{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab #2 for Week 2\n",
    "NLP\n",
    "Rachel Rolle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Data Acquisition–Back translation (1 points)\n",
    "Translate a sentence to your favorite language using Backtranslation with googletrans. For the same sentence, try different languages and print your results. For this task you can use the BackTranslation python library, (https://pypi.org/project/BackTranslation/), which is implemented to back translate the words among any two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rchlr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'french': 'fr'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BackTranslation import BackTranslation\n",
    "trans = BackTranslation()\n",
    "trans.searchLanguage('French')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature has changed drastically in the span of one week.\n",
      "La température a considérablement changé en l'espace d'une semaine.\n",
      "The temperature has changed considerably in the space of a week.\n"
     ]
    }
   ],
   "source": [
    "trans = BackTranslation(url=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.gr',\n",
    "    ])\n",
    "result = trans.translate('The temperature has changed drastically in the span of one week.', src='en', tmp = 'fr')\n",
    "print(result.source_text)\n",
    "print(result.tran_text)\n",
    "print(result.result_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 HTML Parsing and Cleanup (1 points)\n",
    "1. In this exercise we will scrap data from webpages using the python library Beautiful Soup (https://www.crummy.com/software/BeautifulSoup/bs4/doc/). For the first task use StackOverflow as your main source to extract question and best-answer pairs from this website. Identify and extract the question-best answer for the question “What is the module/method used to get the current time?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "\n",
    "try:\n",
    "    req = Request(myurl, headers=headers)\n",
    "\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = response.read()\n",
    "\n",
    "    soupified = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get website title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>datetime - How do I get the current time in Python? - Stack Overflow</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soupified.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " How do I get the current time in Python?\n"
     ]
    }
   ],
   "source": [
    "question = soupified.find('div', {'class': 'question'})\n",
    "questiontext = question.find('div', {'class': 's-prose js-post-body'})\n",
    "print('Question: \\n', questiontext.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best answer: \n",
      " Use datetime:\n",
      ">>> import datetime\n",
      ">>> now = datetime.datetime.now()\n",
      ">>> now\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      ">>> print(now)\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "For just the clock time without the date:\n",
      ">>> now.time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      ">>> print(now.time())\n",
      "15:08:24.789150\n",
      "\n",
      "\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the prefix datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "answer = soupified.find('div', {'class': 'answer'})\n",
    "answertext = answer.find('div', {'class': 's-prose js-post-body'})\n",
    "print('Best answer: \\n', answertext.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Follow similar arguments as in part (a) and write the code to download at your local directory the following file (https://zoisboukouvalas.github.io/COVID19_Twitter_ Dataset.xlsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful!\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = \"https://zoisboukouvalas.github.io/COVID19_Twitter_Dataset.xlsx\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "    soupified = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Check if the download was successful (HTTP status code 200 which is \"200 OK\" like \"404 Not Found\"\n",
    "    if response.status_code == 200:\n",
    "        # If you want to save the file, you can use the following lines:\n",
    "        with open('downloaded_file.xlsx', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print('Download successful!')\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f'Download failed with status code {response.status_code}')\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print('Error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Extract text from pdf files (1 points)\n",
    "Open the sample.pdf file which is available on Canvas and extract the plain text from the document. There are several libraries that can be used for this task such as PyPDF or PDFMiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Simple PDF File \n",
      " This is a small demonstration .pdf file - \n",
      " just for use in the Virtual Mechanics tutorials. More text. And more \n",
      " text. And more text. And more text. And more text. \n",
      " And more text. And more text. And more text. And more text. And more \n",
      " text. And more text. Boring, zzzzz. And more text. And more text. And \n",
      " more text. And more text. And more text. And more text. And more text. \n",
      " And more text. And more text. \n",
      " And more text. And more text. And more text. And more text. And more \n",
      " text. And more text. And more text. Even more. Continued on page 2 ...\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"sample.pdf\")\n",
    "page = reader.pages[0]\n",
    "\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 Extract text from pdf files cont.\n",
    "Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lower case the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ]
    }
   ],
   "source": [
    "corpus = new_corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove digits from the corpus, punctuations, and trailing white-spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = re.sub(r'\\d+', '', corpus)\n",
    "\n",
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenize the corpus using NLTK or Spacy and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\", tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\", tokenized_corpus_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Perform stemming using NLTK and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n",
      "After Stemming:\n",
      "need final demo corpu use notebook done soon done end month notebook run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(tokenized_corpus_without_stopwords)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_without_stopwords:\n",
    "    print(stemmer.stem(word), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Perform lemmatization using NLTK and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization:\n",
      "need finalize demo corpus used notebook done soon done ending month notebook run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"After Lemmatization:\")\n",
    "for word in tokenized_corpus_without_stopwords:\n",
    "    print(lemmatizer.lemmatize(word), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webpage --> Datasets & code --> COVID19_Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re = regular expressions library\n",
    "r = replace\n",
    "\\d+ = one or more digits\n",
    ", corpus then save to new corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
